{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('StudentPerformancePred.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "\n",
    "train['session_id'] = train['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 accuracy: 0.6774901891915623\n",
      "Question 2 accuracy: 0.980691788640401\n",
      "Question 3 accuracy: 0.9363464842070188\n",
      "Question 4 accuracy: 0.7865487793620666\n",
      "Question 5 accuracy: 0.5327763180705748\n",
      "Question 6 accuracy: 0.7475068276022\n",
      "Question 7 accuracy: 0.6942465771929943\n",
      "Question 8 accuracy: 0.555272394872938\n",
      "Question 9 accuracy: 0.7029496592390461\n",
      "Question 10 accuracy: 0.4977737401300903\n",
      "Question 11 accuracy: 0.5593038885249001\n",
      "Question 12 accuracy: 0.8533847570544385\n",
      "Question 13 accuracy: 0.6889441251100409\n",
      "Question 14 accuracy: 0.66581966124286\n",
      "Question 15 accuracy: 0.5064655647941136\n",
      "Question 16 accuracy: 0.6934002472121094\n",
      "Question 17 accuracy: 0.6535079128138277\n",
      "Question 18 accuracy: 0.9473796191852808\n",
      "Overall accuracy: 0.704433807469248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "train = pd.read_csv('StudentPerformancePred.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "\n",
    "train['session_id'] = train['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "for q_no in range(1, 19):\n",
    "    if q_no <= 3:\n",
    "        select = train[train['level_group'] == '0-4']\n",
    "    elif q_no <= 13:\n",
    "        select = train[train['level_group'] == '5-12']\n",
    "    else:\n",
    "        select = train[train['level_group'] == '13-22']\n",
    "\n",
    "    train_X, validate_X = train_test_split(select, test_size=0.2)\n",
    "    train_y = labels.loc[(labels['session'].isin(train_X['session_id'])) & (labels['q'] == q_no)]\n",
    "    validate_y = labels.loc[(labels['session'].isin(validate_X['session_id'])) & (labels['q'] == q_no)]\n",
    "\n",
    "    train_X = train_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_y = train_y['correct']\n",
    "\n",
    "    validate_X = validate_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_y = validate_y['correct']\n",
    "\n",
    "    model = xgb.XGBClassifier(eval_metric='error')\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    # y_pred = model.predict(validate_X)\n",
    "    # print(cross_val_score(validate_y, y_pred, cv=5))\n",
    "\n",
    "    question_accuracy = np.mean(cross_val_score(model, validate_X, validate_y, cv=5))\n",
    "    total_accuracy += question_accuracy\n",
    "    print(\"Question\", q_no, \"accuracy:\", question_accuracy)\n",
    "\n",
    "overall_accuracy = total_accuracy / 18\n",
    "print(\"Overall accuracy:\", overall_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for question 1: {'criterion': 'entropy', 'max_depth': 42, 'min_samples_leaf': 6, 'min_samples_split': 10, 'n_estimators': 342}\n",
      "Question 1 accuracy: 0.7237429444358139\n",
      "Best hyperparameters for question 2: {'criterion': 'gini', 'max_depth': 33, 'min_samples_leaf': 4, 'min_samples_split': 9, 'n_estimators': 333}\n",
      "Question 2 accuracy: 0.9828135800050883\n",
      "Best hyperparameters for question 3: {'criterion': 'entropy', 'max_depth': 21, 'min_samples_leaf': 15, 'min_samples_split': 15, 'n_estimators': 278}\n",
      "Question 3 accuracy: 0.9323149905550565\n",
      "Best hyperparameters for question 4: {'criterion': 'entropy', 'max_depth': 38, 'min_samples_leaf': 13, 'min_samples_split': 13, 'n_estimators': 304}\n",
      "Question 4 accuracy: 0.7984298203546976\n",
      "Best hyperparameters for question 5: {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 6, 'min_samples_split': 15, 'n_estimators': 391}\n",
      "Question 5 accuracy: 0.5533629177333035\n",
      "Best hyperparameters for question 6: {'criterion': 'gini', 'max_depth': 37, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 126}\n",
      "Question 6 accuracy: 0.7806069079799078\n",
      "Best hyperparameters for question 7: {'criterion': 'gini', 'max_depth': 33, 'min_samples_leaf': 14, 'min_samples_split': 4, 'n_estimators': 204}\n",
      "Question 7 accuracy: 0.7400805578257941\n",
      "Best hyperparameters for question 8: {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 13, 'min_samples_split': 5, 'n_estimators': 448}\n",
      "Question 8 accuracy: 0.6189263609612004\n",
      "Best hyperparameters for question 9: {'criterion': 'entropy', 'max_depth': 37, 'min_samples_leaf': 19, 'min_samples_split': 15, 'n_estimators': 449}\n",
      "Question 9 accuracy: 0.7400805578257941\n",
      "Best hyperparameters for question 10: {'criterion': 'entropy', 'max_depth': 22, 'min_samples_leaf': 19, 'min_samples_split': 9, 'n_estimators': 162}\n",
      "Question 10 accuracy: 0.5041334855331383\n",
      "Best hyperparameters for question 11: {'criterion': 'entropy', 'max_depth': 11, 'min_samples_leaf': 12, 'min_samples_split': 18, 'n_estimators': 449}\n",
      "Question 11 accuracy: 0.6418416626702961\n",
      "Best hyperparameters for question 12: {'criterion': 'gini', 'max_depth': 45, 'min_samples_leaf': 1, 'min_samples_split': 8, 'n_estimators': 487}\n",
      "Question 12 accuracy: 0.8567793080312415\n",
      "Best hyperparameters for question 13: {'criterion': 'gini', 'max_depth': 39, 'min_samples_leaf': 6, 'min_samples_split': 4, 'n_estimators': 139}\n",
      "Question 13 accuracy: 0.7303204076072884\n",
      "Best hyperparameters for question 14: {'criterion': 'gini', 'max_depth': 46, 'min_samples_leaf': 8, 'min_samples_split': 4, 'n_estimators': 452}\n",
      "Question 14 accuracy: 0.706344210215849\n",
      "Best hyperparameters for question 15: {'criterion': 'gini', 'max_depth': 11, 'min_samples_leaf': 19, 'min_samples_split': 15, 'n_estimators': 436}\n",
      "Question 15 accuracy: 0.5056210359943533\n",
      "Best hyperparameters for question 16: {'criterion': 'gini', 'max_depth': 36, 'min_samples_leaf': 10, 'min_samples_split': 13, 'n_estimators': 394}\n",
      "Question 16 accuracy: 0.7375345883062818\n",
      "Best hyperparameters for question 17: {'criterion': 'entropy', 'max_depth': 35, 'min_samples_leaf': 8, 'min_samples_split': 4, 'n_estimators': 303}\n",
      "Question 17 accuracy: 0.690006371678228\n",
      "Best hyperparameters for question 18: {'criterion': 'entropy', 'max_depth': 21, 'min_samples_leaf': 18, 'min_samples_split': 7, 'n_estimators': 428}\n",
      "Question 18 accuracy: 0.9471675301078684\n",
      "Overall accuracy: 0.7327837354345114\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "train = pd.read_csv('StudentPerformancePred.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "\n",
    "train['session_id'] = train['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "for q_no in range(1, 19):\n",
    "    if q_no <= 3:\n",
    "        select = train[train['level_group'] == '0-4']\n",
    "    elif q_no <= 13:\n",
    "        select = train[train['level_group'] == '5-12']\n",
    "    else:\n",
    "        select = train[train['level_group'] == '13-22']\n",
    "\n",
    "    train_X, validate_X = train_test_split(select, test_size=0.2)\n",
    "    train_y = labels.loc[(labels['session'].isin(train_X['session_id'])) & (labels['q'] == q_no)]\n",
    "    validate_y = labels.loc[(labels['session'].isin(validate_X['session_id'])) & (labels['q'] == q_no)]\n",
    "\n",
    "    train_X = train_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_y = train_y['correct']\n",
    "\n",
    "    validate_X = validate_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_y = validate_y['correct']\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(1, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "\n",
    "    base_model = RandomForestClassifier()\n",
    "\n",
    "    random_search = RandomizedSearchCV(base_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    random_search.fit(train_X, train_y)\n",
    "\n",
    "    best_params = random_search.best_params_\n",
    "    print(f\"Best hyperparameters for question {q_no}:\", best_params)\n",
    "\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "    best_model.fit(train_X, train_y)\n",
    "\n",
    "    question_accuracy = np.mean(cross_val_score(best_model, validate_X, validate_y, cv=5))\n",
    "    total_accuracy += question_accuracy\n",
    "    print(\"Question\", q_no, \"accuracy:\", question_accuracy)\n",
    "\n",
    "overall_accuracy = total_accuracy / 18\n",
    "print(\"Overall accuracy:\", overall_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 accuracy: 0.7311687639169385\n",
      "Question 2 accuracy: 0.9800552962605229\n",
      "Question 3 accuracy: 0.9310417806476596\n",
      "Question 4 accuracy: 0.7950341436396917\n",
      "Question 5 accuracy: 0.5043525541874084\n",
      "Question 6 accuracy: 0.7702100402338834\n",
      "Question 7 accuracy: 0.7315945181052474\n",
      "Question 8 accuracy: 0.5964390649168193\n",
      "Question 9 accuracy: 0.7239548083655857\n",
      "Question 10 accuracy: 0.4839897512794014\n",
      "Question 11 accuracy: 0.6276280921214086\n",
      "Question 12 accuracy: 0.8629323679002505\n",
      "Question 13 accuracy: 0.7188630944742014\n",
      "Question 14 accuracy: 0.6832154685434974\n",
      "Question 15 accuracy: 0.5069021260691698\n",
      "Question 16 accuracy: 0.7303199573120074\n",
      "Question 17 accuracy: 0.6793974148547911\n",
      "Question 18 accuracy: 0.9509867095347774\n",
      "Overall accuracy: 0.722671441797959\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "train = pd.read_csv('StudentPerformancePred.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "\n",
    "train['session_id'] = train['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "for q_no in range(1, 19):\n",
    "    if q_no <= 3:\n",
    "        select = train[train['level_group'] == '0-4']\n",
    "    elif q_no <= 13:\n",
    "        select = train[train['level_group'] == '5-12']\n",
    "    else:\n",
    "        select = train[train['level_group'] == '13-22']\n",
    "\n",
    "    train_X, validate_X = train_test_split(select, test_size=0.2)\n",
    "    train_y = labels.loc[(labels['session'].isin(train_X['session_id'])) & (labels['q'] == q_no)]\n",
    "    validate_y = labels.loc[(labels['session'].isin(validate_X['session_id'])) & (labels['q'] == q_no)]\n",
    "\n",
    "    train_X = train_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_y = train_y['correct']\n",
    "\n",
    "    validate_X = validate_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_y = validate_y['correct']\n",
    "\n",
    "\n",
    "    best_model = RandomForestClassifier()\n",
    "    best_model.fit(train_X, train_y)\n",
    "\n",
    "    question_accuracy = np.mean(cross_val_score(best_model, validate_X, validate_y, cv=5))\n",
    "    total_accuracy += question_accuracy\n",
    "    print(\"Question\", q_no, \"accuracy:\", question_accuracy)\n",
    "\n",
    "overall_accuracy = total_accuracy / 18\n",
    "print(\"Overall accuracy:\", overall_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 accuracy: 0.743899962400344\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# Assuming you have already read and preprocessed the data\n",
    "train = pd.read_csv('CatBoostData.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# Specify the categorical columns to be used by the CatBoost model\n",
    "cat_features = ['event_name_mode', 'fqid_mode', 'room_fqid_mode', 'text_mode']\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "\n",
    "train['session_id'] = train['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "# Hyperparameter tuning configuration\n",
    "param_dist = {\n",
    "    'iterations': [100, 300, 500, 700],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'depth': [3, 4, 6, 8],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# Loop through questions, create train & validation sets for each question\n",
    "for q_no in range(1, 19):\n",
    "\n",
    "    # Filter dataset based on current question\n",
    "    if q_no <= 3:\n",
    "        select = train[train['level_group'] == '0-4']\n",
    "    elif q_no <= 13:\n",
    "        select = train[train['level_group'] == '5-12']\n",
    "    else:\n",
    "        select = train[train['level_group'] == '13-22']\n",
    "\n",
    "    # Create train & validation sets\n",
    "    train_X, validate_X = train_test_split(select, test_size=0.2)\n",
    "    train_y = labels.loc[(labels['session'].isin(train_X['session_id'])) & (labels['q'] == q_no)]\n",
    "    validate_y = labels.loc[(labels['session'].isin(validate_X['session_id'])) & (labels['q'] == q_no)]\n",
    "\n",
    "    train_X = train_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_y = train_y['correct']\n",
    "\n",
    "    validate_X = validate_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_y = validate_y['correct']\n",
    "\n",
    "    # Hyperparameter tuning for the CatBoost model\n",
    "    catboost_base_model = CatBoostClassifier(cat_features=cat_features)\n",
    "    catboost_random_search = RandomizedSearchCV(catboost_base_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    catboost_random_search.fit(train_X, train_y)\n",
    "    best_catboost_params = catboost_random_search.best_params_\n",
    "\n",
    "    # Train the CatBoost model with the best hyperparameters\n",
    "    model = CatBoostClassifier(cat_features=cat_features, **best_catboost_params)\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    # Calculate the accuracy using cross_val_score\n",
    "    question_accuracy = np.mean(cross_val_score(model, validate_X, validate_y, cv=5))\n",
    "    print(\"Question\", q_no, \"accuracy:\", question_accuracy)\n",
    "    total_accuracy += question_accuracy\n",
    "\n",
    "overall_accuracy = total_accuracy / 18\n",
    "print(\"Overall accuracy:\", overall_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 accuracy: 0.7241671225906389\n",
      "Question 2 accuracy: 0.9762361168336137\n",
      "Question 3 accuracy: 0.9327391687098816\n",
      "Question 4 accuracy: 0.8058560901311035\n",
      "Question 5 accuracy: 0.5588801606653563\n",
      "Question 6 accuracy: 0.7729683239784488\n",
      "Question 7 accuracy: 0.7396568299662504\n",
      "Question 8 accuracy: 0.6216844195581253\n",
      "Question 9 accuracy: 0.7356248860190069\n",
      "Question 10 accuracy: 0.49076894673682264\n",
      "Question 11 accuracy: 0.6475705443844801\n",
      "Question 12 accuracy: 0.8650541592649379\n",
      "Question 13 accuracy: 0.7281986162426011\n",
      "Question 14 accuracy: 0.706131895990796\n",
      "Question 15 accuracy: 0.5228112834991545\n",
      "Question 16 accuracy: 0.7256528718707292\n",
      "Question 17 accuracy: 0.6789730115523254\n",
      "Question 18 accuracy: 0.9548061141093273\n",
      "Overall accuracy: 0.732654475672422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# Assuming you have already read and preprocessed the data\n",
    "train = pd.read_csv('StudentPerformancePred.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "\n",
    "train['session_id'] = train['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "# Hyperparameter tuning configuration\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [-1, 3, 5, 7, 9],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Loop through questions, create train & validation sets for each question\n",
    "for q_no in range(1, 19):\n",
    "\n",
    "    # Filter dataset based on current question\n",
    "    if q_no <= 3:\n",
    "        select = train[train['level_group'] == '0-4']\n",
    "    elif q_no <= 13:\n",
    "        select = train[train['level_group'] == '5-12']\n",
    "    else:\n",
    "        select = train[train['level_group'] == '13-22']\n",
    "\n",
    "    # Create train & validation sets\n",
    "    train_X, validate_X = train_test_split(select, test_size=0.2)\n",
    "    train_y = labels.loc[(labels['session'].isin(train_X['session_id'])) & (labels['q'] == q_no)]\n",
    "    validate_y = labels.loc[(labels['session'].isin(validate_X['session_id'])) & (labels['q'] == q_no)]\n",
    "\n",
    "    train_X = train_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_y = train_y['correct']\n",
    "\n",
    "    validate_X = validate_X.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_y = validate_y['correct']\n",
    "\n",
    "    # Hyperparameter tuning for the LightGBM model\n",
    "    lgbm_base_model = LGBMClassifier()\n",
    "    lgbm_random_search = RandomizedSearchCV(lgbm_base_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    lgbm_random_search.fit(train_X, train_y)\n",
    "    best_lgbm_params = lgbm_random_search.best_params_\n",
    "\n",
    "    # Train the LightGBM model with the best hyperparameters\n",
    "    model = LGBMClassifier(**best_lgbm_params)\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    # Calculate the accuracy using cross_val_score\n",
    "    question_accuracy = np.mean(cross_val_score(model, validate_X, validate_y, cv=5))\n",
    "    total_accuracy += question_accuracy\n",
    "    print(\"Question\", q_no, \"accuracy:\", question_accuracy)\n",
    "\n",
    "overall_accuracy = total_accuracy / 18\n",
    "print(\"Overall accuracy:\", overall_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 accuracy: 0.7165287502652239\n",
      "Question 2 accuracy: 0.9777211966900063\n",
      "Question 3 accuracy: 0.9295565457245916\n",
      "Question 4 accuracy: 0.80500742626777\n",
      "Question 5 accuracy: 0.539147040101846\n",
      "Question 6 accuracy: 0.7644812221514958\n",
      "Question 7 accuracy: 0.7358370464672184\n",
      "Question 8 accuracy: 0.6040738383195416\n",
      "Question 9 accuracy: 0.7265011669849353\n",
      "Question 10 accuracy: 0.5128368342881392\n",
      "Question 11 accuracy: 0.6346276257161044\n",
      "Question 12 accuracy: 0.8663271801400382\n",
      "Question 13 accuracy: 0.7224697644812221\n",
      "Question 14 accuracy: 0.7008274984086569\n",
      "Question 15 accuracy: 0.4988330150647146\n",
      "Question 16 accuracy: 0.7388075535752174\n",
      "Question 17 accuracy: 0.6893698281349459\n",
      "Question 18 accuracy: 0.954169318905156\n",
      "Overall accuracy: 0.7287290473159348\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "train_other = pd.read_csv('StudentPerformancePred.csv')\n",
    "train_catboost = pd.read_csv('CatBoostData.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]))\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]))\n",
    "\n",
    "train_other['session_id'] = train_other['session_id'].astype(str)\n",
    "train_catboost['session_id'] = train_catboost['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)\n",
    "\n",
    "cat_features = ['event_name_mode', 'fqid_mode', 'room_fqid_mode', 'text_mode']\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "for q_no in range(1, 19):\n",
    "    if q_no <= 3:\n",
    "        select_other = train_other[train_other['level_group'] == '0-4']\n",
    "        select_catboost = train_catboost[train_catboost['level_group'] == '0-4']\n",
    "    elif q_no <= 13:\n",
    "        select_other = train_other[train_other['level_group'] == '5-12']\n",
    "        select_catboost = train_catboost[train_catboost['level_group'] == '5-12']\n",
    "    else:\n",
    "        select_other = train_other[train_other['level_group'] == '13-22']\n",
    "        select_catboost = train_catboost[train_catboost['level_group'] == '13-22']\n",
    "\n",
    "    train_X_other, validate_X_other = train_test_split(select_other, test_size=0.2)\n",
    "    train_X_catboost, validate_X_catboost = train_test_split(select_catboost, test_size=0.2)\n",
    "\n",
    "    train_y = labels.loc[(labels['session'].isin(train_X_other['session_id'])) & (labels['q'] == q_no)]\n",
    "    validate_y = labels.loc[(labels['session'].isin(validate_X_other['session_id'])) & (labels['q'] == q_no)]\n",
    "\n",
    "    train_X_other = train_X_other.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_X_catboost = train_X_catboost.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_y = train_y['correct']\n",
    "\n",
    "    validate_X_other = validate_X_other.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_X_catboost = validate_X_catboost.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_y = validate_y['correct']\n",
    "\n",
    "    # Train the four models\n",
    "    rf_model = RandomForestClassifier(n_estimators=100)\n",
    "    xgb_model = XGBClassifier()\n",
    "    cat_model = CatBoostClassifier(cat_features=cat_features, iterations=500, learning_rate=0.1, depth=6, verbose=0)\n",
    "    lgbm_model = LGBMClassifier()\n",
    "\n",
    "    # Create the voting classifier\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[('rf', rf_model), ('xgb', xgb_model), ('cat', cat_model), ('lgbm', lgbm_model)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    rf_model.fit(train_X_other, train_y)\n",
    "    xgb_model.fit(train_X_other, train_y)\n",
    "    cat_model.fit(train_X_catboost, train_y)\n",
    "    lgbm_model.fit(train_X_other, train_y)\n",
    "\n",
    "    # Obtain the predictions\n",
    "    rf_pred = rf_model.predict_proba(validate_X_other)[:, 1]\n",
    "    xgb_pred = xgb_model.predict_proba(validate_X_other)[:, 1]\n",
    "    cat_pred = cat_model.predict_proba(validate_X_catboost)[:, 1]\n",
    "    lgbm_pred = lgbm_model.predict_proba(validate_X_other)[:, 1]\n",
    "\n",
    "    # Average the predictions\n",
    "    ensemble_pred = (rf_pred + xgb_pred + cat_pred + lgbm_pred) / 4\n",
    "    ensemble_pred = np.round(ensemble_pred).astype(int)\n",
    "\n",
    "    # Calculate the accuracy of the ensemble model\n",
    "    question_accuracy = np.mean(ensemble_pred == validate_y)\n",
    "    total_accuracy += question_accuracy\n",
    "    print(\"Question\", q_no, \"accuracy:\", question_accuracy)\n",
    "\n",
    "overall_accuracy = total_accuracy / 18\n",
    "print(\"Overall accuracy:\", overall_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m# Train the CatBoost model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m catboost_model \u001b[39m=\u001b[39m CatBoostClassifier(cat_features\u001b[39m=\u001b[39mcat_features, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbest_catboost_params)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m catboost_model\u001b[39m.\u001b[39;49mfit(train_X_catboost, train_y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39m# Create the ensemble model with weights\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m ensemble_model \u001b[39m=\u001b[39m VotingClassifier(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m     estimators\u001b[39m=\u001b[39m[\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mrf\u001b[39m\u001b[39m'\u001b[39m, rf_model),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     weights\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m],  \u001b[39m# Higher weight for the random forest model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/brodybarton/Documents/GitHub/student-performance-prediction/model.ipynb#X11sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/catboost_env/lib/python3.8/site-packages/catboost/core.py:5128\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5126\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5128\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[1;32m   5129\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[1;32m   5130\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[1;32m   5131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/catboost_env/lib/python3.8/site-packages/catboost/core.py:2355\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2351\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2353\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2354\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2356\u001b[0m         train_pool,\n\u001b[1;32m   2357\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2358\u001b[0m         params,\n\u001b[1;32m   2359\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2360\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2361\u001b[0m     )\n\u001b[1;32m   2363\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/catboost_env/lib/python3.8/site-packages/catboost/core.py:1759\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1759\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1760\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4623\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4672\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "train = pd.read_csv('StudentPerformancePred.csv')\n",
    "train_catboost = pd.read_csv('CatBoostData.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]))\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]))\n",
    "\n",
    "train['session_id'] = train['session_id'].astype(str)\n",
    "labels['session'] = labels['session'].astype(str)\n",
    "\n",
    "cat_features = ['event_name_mode', 'fqid_mode', 'room_fqid_mode', 'text_mode']\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "for q_no in range(1, 19):\n",
    "    # Select train data based on current question\n",
    "    if q_no <= 3:\n",
    "        select = train[train['level_group'] == '0-4']\n",
    "        select_catboost = train_catboost[train_catboost['level_group'] == '0-4']\n",
    "    elif q_no <= 13:\n",
    "        select = train[train['level_group'] == '5-12']\n",
    "        select_catboost = train_catboost[train_catboost['level_group'] == '5-12']\n",
    "    else:\n",
    "        select = train[train['level_group'] == '13-22']\n",
    "        select_catboost = train_catboost[train_catboost['level_group'] == '13-22']\n",
    "\n",
    "    # Create train & validation sets\n",
    "    train_X_other, validate_X_other = train_test_split(select, test_size=0.2)\n",
    "    train_X_catboost, validate_X_catboost = train_test_split(select_catboost, test_size=0.2)\n",
    "    train_y = labels.loc[(labels['session'].isin(train_X_other['session_id'])) & (labels['q'] == q_no)]\n",
    "    validate_y = labels.loc[(labels['session'].isin(validate_X_other['session_id'])) & (labels['q'] == q_no)]\n",
    "\n",
    "    train_X_other = train_X_other.drop(['session_id', 'level_group'], axis=1)\n",
    "    train_y = train_y['correct']\n",
    "\n",
    "    validate_X_other = validate_X_other.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_y = validate_y['correct']\n",
    "\n",
    "    train_X_catboost = train_X_catboost.drop(['session_id', 'level_group'], axis=1)\n",
    "    validate_X_catboost = validate_X_catboost.drop(['session_id', 'level_group'], axis=1)\n",
    "# Define hyperparameter distributions for each model\n",
    "rf_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(1, 50),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(1, 50),\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': np.linspace(0, 1, 10),\n",
    "}\n",
    "\n",
    "lgbm_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(1, 50),\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'num_leaves': randint(20, 200),\n",
    "    'min_child_samples': randint(1, 100),\n",
    "}\n",
    "\n",
    "catboost_param_dist = {\n",
    "    'iterations': randint(100, 500),\n",
    "    'depth': randint(1, 16),\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'l2_leaf_reg': randint(2, 30),\n",
    "    'border_count': randint(1, 255),\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "rf_random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=rf_param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "rf_random_search.fit(train_X_other, train_y)\n",
    "best_rf_params = rf_random_search.best_params_\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(XGBClassifier(), param_distributions=xgb_param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "xgb_random_search.fit(train_X_other, train_y)\n",
    "best_xgb_params = xgb_random_search.best_params_\n",
    "\n",
    "lgbm_random_search = RandomizedSearchCV(LGBMClassifier(), param_distributions=lgbm_param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "lgbm_random_search.fit(train_X_other, train_y)\n",
    "best_lgbm_params = lgbm_random_search.best_params_\n",
    "\n",
    "catboost_random_search = RandomizedSearchCV(CatBoostClassifier(cat_features=cat_features, verbose=0), param_distributions=catboost_param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "catboost_random_search.fit(train_X_catboost, train_y)\n",
    "best_catboost_params = catboost_random_search.best_params_\n",
    "\n",
    "# Train the models with the best hyperparameters\n",
    "rf_model = RandomForestClassifier(**best_rf_params)\n",
    "rf_model.fit(train_X_other, train_y)\n",
    "\n",
    "xgb_model = XGBClassifier(**best_xgb_params)\n",
    "xgb_model.fit(train_X_other, train_y)\n",
    "\n",
    "lgbm_model = LGBMClassifier(**best_lgbm_params)\n",
    "lgbm_model.fit(train_X_other, train_y)\n",
    "   \n",
    "# Train the CatBoost model\n",
    "catboost_model = CatBoostClassifier(cat_features=cat_features, verbose=0, **best_catboost_params)\n",
    "catboost_model.fit(train_X_catboost, train_y)\n",
    "\n",
    "# Create the ensemble model with weights\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgbm', lgbm_model),\n",
    "        ('catboost', catboost_model),\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[3, 1, 1, 1],  # Higher weight for the random forest model\n",
    ")\n",
    "\n",
    "# Assuming train_X_other and validate_X_other have the same features\n",
    "train_X_catboost = train_X_catboost[train_X_other.columns]\n",
    "validate_X_catboost = validate_X_catboost[train_X_other.columns]\n",
    "\n",
    "\n",
    "# Fit the ensemble model\n",
    "ensemble_model.fit(train_X_other, train_y)\n",
    "\n",
    " # Calculate the accuracy using cross_val_score\n",
    "accuracy = np.mean(cross_val_score(ensemble_model, validate_X_other, validate_y, cv=5))\n",
    "total_accuracy += accuracy\n",
    "print(\"Question\", q_no, \"accuracy:\", accuracy)\n",
    "\n",
    "print(\"Total accuracy:\", total_accuracy / 18)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
